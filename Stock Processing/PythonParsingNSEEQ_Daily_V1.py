# -*- coding: utf-8 -*-
"""
Created on Tue Dec 10 17:07:11 2019

@author: eamalok

'1.load the downloaded file and create a merged file
'2.Calculate the required moving averages
'3.Create the points for the symbol

EQData is proceesed table
EQBullish is Processed pivort table
"""
import os
import glob
import pandas as pd
import numpy as np
#import datetime
import time

starttime=time.time()

WithFO="Yes" #mention "Yes" if analysis done only FO series else "No"


#To combine the csv files in specific folder


#Defining rolling function
def get_rolling(group, freq, testa,funct):
    if funct == 'mean':
        return group.rolling(freq, on='TIMESTAMP',min_periods=1)[testa].mean()
    elif funct == "max":
        return group.rolling(freq, on='TIMESTAMP',min_periods=1)[testa].max()
    elif funct == 'min':
        return group.rolling(freq, on='TIMESTAMP',min_periods=1)[testa].min()
    elif funct == 'sum':
        return group.rolling(freq, on='TIMESTAMP',min_periods=1)[testa].sum()
    elif funct == 'std':
        return group.rolling(freq, on='TIMESTAMP',min_periods=1)[testa].std()
    elif funct == 'count':
        return group.rolling(freq, on='TIMESTAMP',min_periods=1)[testa].count()
    else :
        return 0

##Main code
os.chdir("C:/nse_download/download")
path='C:/nse_download/Parsed_Output/'
#os.getcwd()

extension = 'csv'
all_filenames = [i for i in glob.glob('*.{}'.format(extension))]
combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames],sort=False)

##To save the combined file to Specific folder removing last column

combined_csv.drop(combined_csv.columns[combined_csv.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)
#combined_csv=combined_csv.iloc[:,:-1]
combined_csv['TIMESTAMP'] = pd.to_datetime(combined_csv['TIMESTAMP'])
combined_csv['Year'] = combined_csv['TIMESTAMP'].apply(lambda x: str(x.year))
combined_csv['Month'] = combined_csv['TIMESTAMP'].apply(lambda x: str(x.year)+"_"+str("{0:0=2d}".format(x.month)))


#Clearing the data for stocks less number of trades & average price trading below 10
combined_csv['CountofDays']=combined_csv.groupby(['SYMBOL','Year'])['CLOSE'].transform('count')
combined_csv['AvgTradingPrice']=combined_csv.groupby(['SYMBOL','Year'])['CLOSE'].transform('mean')
combined_csv['CountofDays_Baseline']=0.5*combined_csv.groupby(['Year'])['CountofDays'].transform('mean')
combined_csv['CountofDays_Baseline_delete']=combined_csv['CountofDays']-combined_csv['CountofDays_Baseline']

combined_csv=combined_csv[combined_csv["CountofDays_Baseline_delete"] > 0]
combined_csv=combined_csv[combined_csv["AvgTradingPrice"] > 5]
combined_csv.drop(columns=['Year','Month','CountofDays', 'AvgTradingPrice', 'CountofDays_Baseline', 'CountofDays_Baseline_delete'],inplace=True)
#combined_csv.to_csv(path+"combined_csv.csv", index=False, encoding='utf-8-sig')

##read the combined file
#EQData=pd.read_csv(path+"combined_csv.csv") 
EQData=combined_csv.reset_index(drop=True)
#EQData.dtypes
EQData=EQData[EQData.SERIES=='EQ']

##converting the data into time series and creating the week number and sorting wrt date
EQData['TIMESTAMP'] = pd.to_datetime(EQData['TIMESTAMP'])
#EQData['Week_Number'] = EQData['TIMESTAMP'].apply(lambda x: str(x.year)+"_"+str("{0:0=2d}".format(x.weekofyear)))
#EQData['Week_Number'] = EQData['TIMESTAMP'].apply(lambda x: x.weekofyear)
EQData['Year'] = EQData['TIMESTAMP'].apply(lambda x: str(x.year))
EQData['Month'] = EQData['TIMESTAMP'].apply(lambda x: str(x.year)+"_"+str("{0:0=2d}".format(x.month)))

EQData["YearWeek"] = EQData['TIMESTAMP'].apply(lambda x: x.strftime("%W")) #%W for monday start week, %U for sunday start week
EQData["YearWeekITU"] = EQData['TIMESTAMP'].apply(lambda x: x.weekofyear)

EQData['YearForWeekNo']=np.where(EQData['YearWeek'].astype(int)>EQData['YearWeekITU'], EQData['Year'].astype(int)+1, np.where((EQData['YearWeek'].astype(int)+15)<EQData['YearWeekITU'], EQData['Year'].astype(int)-1,EQData['Year']))

EQData['Week_Number'] = EQData['YearForWeekNo'].astype(str)+"_"+EQData["YearWeekITU"].astype(str).str.zfill(2)

EQData.drop(columns=['YearWeek','YearWeekITU','YearForWeekNo'],inplace=True)


#reset index with sorting
EQData = EQData.sort_values(by='TIMESTAMP', ascending=True)
EQData.reset_index(drop=True,inplace=True)

#EQData[(EQData.SYMBOL=='3MINDIA') ].head(300)
#EQData[(EQData.SYMBOL=='3MINDIA') | (EQData.SYMBOL=='ABCAPITAL')].head(300)

#To filter the F&O stocks-----Special Check
#Result can be generated by NSE_FO_LIST_GENERATION.py
if WithFO=="Yes" :
    FOBaseFileData=pd.read_csv(path+"combined_F&OSeries_Basefile.csv")
    #FOBaseFileData.drop(FOBaseFileData.columns.difference(['SYMBOL','Year','FOMode']), axis=1, inplace=True)
    #FOBaseFileData['Year']=FOBaseFileData['Year'].astype(str)
    #EQData['Year']=EQData['Year'].astype(str)
    #EQData = pd.merge(EQData, FOBaseFileData, how='left', left_on=['SYMBOL','Year'], right_on = ['SYMBOL','Year']) #use how='left' for left join, inner for overlap
    FOBaseFileData.drop(FOBaseFileData.columns.difference(['SYMBOL','FOMode']), axis=1, inplace=True)
    EQData = pd.merge(EQData, FOBaseFileData, how='left', left_on=['SYMBOL'], right_on = ['SYMBOL']) #use how='left' for left join, inner for overlap

##RSI calculation
print("RSI")
EQData['CLOSE_PERCENT']=(100*(EQData['CLOSE']-EQData['PREVCLOSE'])/EQData['PREVCLOSE'])
EQData['UpwardMovement']=np.where(EQData['CLOSE_PERCENT']>=0, EQData['CLOSE_PERCENT'], 0)
EQData['DownwardMovement']=np.where(EQData['CLOSE_PERCENT']<=0, -1*EQData['CLOSE_PERCENT'], 0)
print("DownwardMovement")
EQData['20D_UpwardMovement']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,20,'UpwardMovement','mean')
print("20D1")
EQData['20D_DownwardMovement']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,20,'DownwardMovement','mean')
print("20D2")
EQData['RelativeStrength'] =EQData['20D_UpwardMovement']/EQData['20D_DownwardMovement']
EQData['RSI']=100-100/(1+EQData['RelativeStrength'])
EQData['2DSTD'] =EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,2,'CLOSE','std')
EQData['BB_LOW_20MA_2STD']=EQData['CLOSE']-EQData['2DSTD']
EQData['BB_HIGH_20MA_2STD']=EQData['CLOSE']+EQData['2DSTD']
EQData['CLOSE_20D_shifted'] = EQData.groupby(['SYMBOL'])['CLOSE'].shift(20)
EQData['CLOSE_365D_shifted'] = EQData.groupby(['SYMBOL'])['CLOSE'].shift(365)
EQData['CLOSE_20D_shifted_percent']=100*(EQData['CLOSE']-EQData['CLOSE_20D_shifted'])/(EQData['CLOSE_20D_shifted'])

EQData['CountofDaysinYearCummulative'] = EQData.groupby(['SYMBOL','Year'])['CLOSE'].cumcount().astype(int)

##Moving average
print("MA")
EQData['5D_High']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,5,'HIGH','max')
EQData['5D_Low']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,5,'LOW','min')
EQData['20DMA']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,20,'CLOSE','mean')
EQData['20D_High']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,20,'HIGH','max')
EQData['20D_Low']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,20,'LOW','min')
EQData['20D_Mid']=(EQData['20D_High']+EQData['20D_Low'])/2
EQData['365D_High']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,365,'HIGH','max')
EQData['365D_Low']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,365,'LOW','min')
EQData['9DMA']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,9,'CLOSE','mean')
EQData['20D_Volume']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,20,'TOTTRDQTY','mean')
EQData['2D_Low']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,2,'CLOSE','min')
EQData['2D_High']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,2,'CLOSE','max')
EQData['20D_CLOSE_PERCENT']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,20,'CLOSE_PERCENT','sum')
EQData['365D_Max20D_SHIFTED%_Drawdown']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,20,'CLOSE_20D_shifted_percent','min')
EQData['365D_Max20D_SHIFTED%_MaxGain']=EQData.groupby('SYMBOL',as_index=False,group_keys=False).apply(get_rolling,20,'CLOSE_20D_shifted_percent','max')


##Lifetime highs
print("LH")
EQData['Life_High']=EQData.groupby('SYMBOL').HIGH.cummax()
EQData['Life_LOW']=EQData.groupby('SYMBOL').LOW.cummin()

EQData['%Above_Life_LOW']=100*(EQData['CLOSE']-EQData['Life_LOW'])/(EQData['CLOSE'])
EQData['%Below_Life_High']=100*(EQData['Life_High']-EQData['CLOSE'])/(EQData['CLOSE'])
EQData['%Above_365D_LOW']=100*(EQData['CLOSE']-EQData['365D_Low'])/(EQData['CLOSE'])
EQData['%Below_365D_High']=100*(EQData['365D_High']-EQData['CLOSE'])/(EQData['CLOSE'])

##Pivot points
EQData['P_LOW']=EQData.groupby(['SYMBOL'])['LOW'].shift(1)
EQData['P_HIGH']=EQData.groupby(['SYMBOL'])['HIGH'].shift(1)
EQData['P_CLOSE']=EQData.groupby(['SYMBOL'])['CLOSE'].shift(1)

EQData['PP']=(EQData['P_HIGH']+EQData['P_LOW']+EQData['P_CLOSE'])/3 #Pivot point = (H + L + C) / 3
EQData['S1']=2*EQData['PP']-EQData['P_HIGH'] #First support level (S1) = (2 * P) – H
EQData['S2']=EQData['PP']-1*(EQData['P_HIGH']-EQData['P_LOW']) #Second support level (S2) = P – (H – L)
EQData['S3']=EQData['PP']-2*(EQData['P_HIGH']-EQData['P_LOW']) #Third support level (S3) = P – 2*(H-L)
EQData['S4']=EQData['PP']-3*(EQData['P_HIGH']-EQData['P_LOW']) #Third support level (S4) = P – 3*(H-L)
EQData['S5']=EQData['PP']-4*(EQData['P_HIGH']-EQData['P_LOW']) #Third support level (S5) = P – 4*(H-L)
EQData['R1']=2*EQData['PP']-EQData['P_LOW'] #First resistance level (R1) = (2 * P) – L
EQData['R2']=EQData['PP']+1*(EQData['P_HIGH']-EQData['P_LOW']) #Second resistance level (R2) = P + (H – L)
EQData['R3']=EQData['PP']+2*(EQData['P_HIGH']-EQData['P_LOW']) #Third resistance level (R3) = P + 2*(H – L)
EQData['R4']=EQData['PP']+3*(EQData['P_HIGH']-EQData['P_LOW']) #Third resistance level (R4) = P + 3*(H – L)
EQData['R5']=EQData['PP']+4*(EQData['P_HIGH']-EQData['P_LOW']) #Third resistance level (R5) = P + 4*(H – L)


##PIVOT POINT RANK
#EQData['20D_High'] 'Donchain high 20days
#EQData['20D_Low'] ' Donchain low 20days
EQData['PIVOT_Bulish_Rank']=np.where(EQData['CLOSE']>EQData['R1'], 1, 0)+np.where(EQData['CLOSE']>EQData['R2'], 1, 0)+np.where(EQData['CLOSE']>EQData['R3'], 1, 0)+np.where(EQData['CLOSE']>EQData['R4'], 1, 0)+np.where(EQData['CLOSE']>EQData['R5'], 1, 0)
EQData['PIVOT_Bearish_Rank']=np.where(EQData['CLOSE']<EQData['S1'], 1, 0)+np.where(EQData['CLOSE']<EQData['S2'], 1, 0)+np.where(EQData['CLOSE']<EQData['S3'], 1, 0)+np.where(EQData['CLOSE']<EQData['S4'], 1, 0)+np.where(EQData['CLOSE']<EQData['S5'], 1, 0)


##Donchain Channel Rank
#EQData['20D_High'] 'Donchain high 20days
#EQData['20D_Low'] ' Donchain low 20days
#EQData['20D_Mid'] ' Donchain mid 20days
EQData['DC_Bulish_Rank']=np.where(EQData['CLOSE']>EQData['20D_Mid'], 1, 0)+np.where(EQData['HIGH']==EQData['20D_High'], 1, 0)
EQData['DC_Bearish_Rank']=np.where(EQData['CLOSE']<EQData['20D_Mid'], 1, 0)+np.where(EQData['LOW']==EQData['20D_Low'], 1, 0)

##KEY INDICATOR NETWORK HEALTH
#5Day Close
EQData['NSE_BULLISH_CLOSE>5DMA_MAX']=np.where(EQData['CLOSE']>=EQData['5D_High'], 1, 0)
EQData['NSE_BEARISH_CLOSE<5DMA_MIN']=np.where(EQData['CLOSE']<=EQData['5D_Low'], 1, 0)

#PIVOT Close
EQData['NSE_BULLISH_CLOSE>R1']=np.where(EQData['CLOSE']>EQData['R1'], 1, 0)
EQData['NSE_BEARISH_CLOSE<S1']=np.where(EQData['CLOSE']<EQData['S1'], 1, 0)

#DROP PIVOT MEASUREMENTS
EQData.drop(columns=['P_LOW','P_HIGH','P_CLOSE','PP','S1','S2','S3','S4','S5','R1','R2','R3','R4','R5'],inplace=True)


##To calculate the EQBullish values
print("EQBA")
EQData['EQBullish_CLOSE>20DMA']=np.where(EQData['CLOSE']>=EQData['20DMA'], 1, 0)
EQData['EQBullish_LOW>20DMA']=np.where(EQData['LOW']>=EQData['20DMA'], 1, 0)
EQData['EQBullish_High>20DMA']=np.where(EQData['HIGH']>=EQData['20DMA'], 1, -3)
EQData['EQBullish_High=365D_High']=np.where(EQData['HIGH']==EQData['365D_High'], 3, 0)
EQData['EQBullish_Vol>2_20D_Volume']=np.where(np.logical_and(EQData['TOTTRDQTY']>=2*EQData['20D_Volume'], EQData['UpwardMovement']>0), 1, 0)
print("EQBA1")
EQData['HIGH>2D_High_close>20DMA']=np.where(np.logical_and(EQData['HIGH']>=EQData['2D_High'], EQData['CLOSE']>=EQData['20DMA']), 1, 0)
EQData['LOW>2D_Low_close>20DMA']=np.where(np.logical_and(EQData['LOW']>EQData['2D_Low'],EQData['CLOSE']>=EQData['20DMA']), 1, 0)
EQData['CLOSE>CLOSE_20D_shifted']=np.where(EQData['CLOSE']>EQData['CLOSE_20D_shifted'], 1, 0)
print("EQBA2")
EQData['<5%_%Below_365D_High']=np.where(EQData['%Below_365D_High']<5, 2, 0)
EQData['>20%Away_365D_High']=np.where(EQData['%Below_365D_High']>20, -3, 0)
print("EQBA3")
#EQData['RSI>60_CLOSE20Dshifted>5%']=np.where(np.logical_and(EQData['RSI']>=60, EQData['CLOSE_20D_shifted_percent']>3,EQData['CLOSE']>=EQData['20DMA']), 1, 0)
EQData['RSI>60_CLOSE20Dshifted>5%']=np.where((EQData['RSI']>=60)&(EQData['CLOSE_20D_shifted_percent']>3)&(EQData['CLOSE']>=EQData['20DMA']), 1, 0)
print("EQBA4")
EQData['Close>BB20MA_CLOSE20Dshifted>5%']=np.where(np.logical_and(EQData['CLOSE']>=EQData['BB_HIGH_20MA_2STD'], EQData['CLOSE_20D_shifted_percent']>5), 1, 0)

print("ULTRABULL")
EQData['UltraBull_Vol_5']=EQData['EQBullish_CLOSE>20DMA']+EQData['EQBullish_LOW>20DMA']+EQData['EQBullish_High>20DMA']+EQData['EQBullish_Vol>2_20D_Volume']+EQData['HIGH>2D_High_close>20DMA']
EQData['UltraBull_Vol_Low_6']=EQData['UltraBull_Vol_5']+EQData['LOW>2D_Low_close>20DMA']+EQData['EQBullish_High=365D_High']
EQData['UltraBull_Vol_Low_1YRhigh_7']=EQData['UltraBull_Vol_Low_6']+EQData['<5%_%Below_365D_High']+EQData['>20%Away_365D_High']
EQData['UltraBull_RSI_UPBB_BAND_8']=EQData['UltraBull_Vol_Low_1YRhigh_7']+EQData['RSI>60_CLOSE20Dshifted>5%']+EQData['Close>BB20MA_CLOSE20Dshifted>5%']
print("ULTRABULLend")
##----Choose which export we need to analyse in excel
##To export the full table to csv
###<-><-><-><-><-><-><->###EQData.to_csv(path+"combined_csv_processed_output.csv", index=False, encoding='utf-8-sig')

##--To export the data as years_Excluding the first year for the purpose of 365 days max
EQDataYear=EQData.Year.drop_duplicates().sort_values(ascending=True).reset_index(drop=True)
for i in EQDataYear[1:]:
    EQData[EQData.Year == i].to_csv(path+i+"combined_csv_processed_output.csv", index=False, encoding='utf-8-sig')
#To export the last 30 days table to csv
#EQData[EQData.TIMESTAMP > datetime.datetime.now()- pd.to_timedelta("30day")].to_csv(path+"combined_csv_processed_output.csv", index=False, encoding='utf-8-sig')

#To export the last 30 days for specific symbols to csv
#EQData[((EQData.SYMBOL=='3MINDIA') | (EQData.SYMBOL=='ABCAPITAL'))&(EQData.TIMESTAMP > datetime.datetime.now()- pd.to_timedelta("30day" ))].to_csv(path+"combined_csv_processed_output.csv")

##----Creating a pivort table
#pd.pivot_table(df, values = 'Value', index=['Country','Year'], columns = 'Indicator').reset_index()
print("Pivot")
EQBullish=pd.pivot_table(EQData, values = 'UltraBull_RSI_UPBB_BAND_8', index=['SYMBOL'], columns = 'Week_Number',aggfunc=['mean'],margins=True).reset_index()
EQBullishYear=pd.pivot_table(EQData, values = 'UltraBull_RSI_UPBB_BAND_8', index=['SYMBOL'], columns = 'Year',aggfunc=['mean'],margins=True).reset_index()
EQBullishMonth=pd.pivot_table(EQData, values = 'UltraBull_RSI_UPBB_BAND_8', index=['SYMBOL'], columns = 'Month',aggfunc=['mean'],margins=True).reset_index()

#Close updates
EQDataWeekRaw=EQData.drop_duplicates(['SYMBOL','Week_Number'],keep= 'last')
EQDataMonthRaw=EQData.drop_duplicates(['SYMBOL','Month'],keep= 'last')
EQDataYearRaw=EQData.drop_duplicates(['SYMBOL','Year'],keep= 'last')

EQBullishClose=pd.pivot_table(EQDataWeekRaw, values = 'CLOSE', index=['SYMBOL'], columns = 'Week_Number',aggfunc=[np.mean],margins=True).reset_index()
EQBullishCloseYear=pd.pivot_table(EQDataYearRaw, values = 'CLOSE', index=['SYMBOL'], columns = 'Year',aggfunc=[np.mean],margins=True).reset_index()
EQBullishCloseMonth=pd.pivot_table(EQDataMonthRaw, values = 'CLOSE', index=['SYMBOL'], columns = 'Month',aggfunc=[np.mean],margins=True).reset_index()
#to set symbol as index
EQBullish.set_index('SYMBOL', inplace = True)
EQBullishYear.set_index('SYMBOL', inplace = True)
EQBullishMonth.set_index('SYMBOL', inplace = True)
EQBullishClose.set_index('SYMBOL', inplace = True)
EQBullishCloseYear.set_index('SYMBOL', inplace = True)
EQBullishCloseMonth.set_index('SYMBOL', inplace = True)

#to drop last row
EQBullish.drop(EQBullish.tail(1).index,inplace=True)
EQBullishYear.drop(EQBullishYear.tail(1).index,inplace=True)
EQBullishMonth.drop(EQBullishMonth.tail(1).index,inplace=True)
EQBullishClose.drop(EQBullishClose.tail(1).index,inplace=True)
EQBullishCloseMonth.drop(EQBullishCloseMonth.tail(1).index,inplace=True)
EQBullishCloseYear.drop(EQBullishCloseYear.tail(1).index,inplace=True)
#to drop top index with sum
EQBullish.columns = EQBullish.columns.droplevel(0)
EQBullishYear.columns = EQBullishYear.columns.droplevel(0)
EQBullishMonth.columns = EQBullishMonth.columns.droplevel(0)
EQBullishClose.columns = EQBullishClose.columns.droplevel(0)
EQBullishCloseYear.columns = EQBullishCloseYear.columns.droplevel(0)
EQBullishCloseMonth.columns = EQBullishCloseMonth.columns.droplevel(0)

##---export the pivort table to the csv
###<-><-><-><-><-><-><->###EQBullish.to_csv(path+"combined_csv_processed_output_pivort.csv")

##---To see the last 6 weeks and total
###<-><-><-><-><-><-><->###EQBullish[EQBullish.columns[-7:]].to_csv(path+"combined_csv_processed_output_pivort.csv")
print("pivot kpi")
EQBullish['2W']=EQBullish.iloc[:,-3:-1].sum(axis=1)
EQBullish['4W']=EQBullish.iloc[:,-6:-2].sum(axis=1)
EQBullish['12W']=EQBullish.iloc[:,-15:-3].sum(axis=1)
EQBullish['2WAvg']=EQBullish['2W']/2
EQBullish['4WAvg']=EQBullish['4W']/4
EQBullish['12WAvg']=EQBullish['12W']/12
EQBullish['BullCriteria']=np.where(np.logical_and(EQBullish['2WAvg']>=EQBullish['4WAvg'], EQBullish['4WAvg']>=EQBullish['12WAvg']), 1, 0)

###<-><-><-><-><-><-><->###EQBullish[EQBullish.columns[-20:]].to_csv(path+"combined_csv_processed_output_pivort.csv")
print("pivot export")
EQBullish.to_csv(path+"combined_csv_processed_output_pivort_week.csv", encoding='utf-8-sig')
EQBullishYear.to_csv(path+"combined_csv_processed_output_pivort_Year.csv", encoding='utf-8-sig')
EQBullishMonth.to_csv(path+"combined_csv_processed_output_pivort_Month.csv", encoding='utf-8-sig')
EQBullishClose.to_csv(path+"combined_csv_processed_output_close_pivort_week.csv", encoding='utf-8-sig')
EQBullishCloseYear.to_csv(path+"combined_csv_processed_output_close_pivort_Year.csv", encoding='utf-8-sig')
EQBullishCloseMonth.to_csv(path+"combined_csv_processed_output_close_pivort_Month.csv", encoding='utf-8-sig')

Endtime=time.time()

print("Starttime:",starttime,"\n","Endtime:",Endtime,"\n","DeltatimeSec:",Endtime-starttime,"\n","DeltatimeMin:",(Endtime-starttime)/60)

